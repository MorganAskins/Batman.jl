{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Poisson likelihood calculator\n",
    "\n",
    "Given $M$ components, each with an estimated rate $\\vec{\\beta}$ determined by a normal distribution with uncertainty $\\vec{\\sigma}$, calculate the confidence itervals and perform a hypothesis tests for each parameter $b$.\n",
    "\n",
    "Nominally each event corresponds to a set of observables $\\vec{x}$ of $N$ measurements, for any given measurement, the probability for that particular measurement to come from a particular components is given by\n",
    "\n",
    "$$ P_i(\\vec{x}) $$\n",
    "\n",
    "The prior probability is then formed through a combination of these components such that the total probability is \n",
    "\n",
    "$$ \\mathbf{P} = \\sum_i^M P_i(\\vec{x}) $$\n",
    "\n",
    "The likelihood for a full data set of $N$ measurements is the product of each event total probability\n",
    "\n",
    "$$\\mathcal{L}(\\vec{x}) = \\prod_j^N \\left( \\sum_i^M b_iP_i(\\vec{x}) \\right) / \\sum_i^Mb_i $$\n",
    "\n",
    "We can extend the likelihood by proclaiming that each components as well as the sum of components are simply a stochastic process, produces the extended likelihood:\n",
    "\n",
    "$$\\mathcal{L}(\\vec{x}) = \\frac{\\text{e}^{-\\sum_i^Mb_i}}{N!} \\prod_j^N \\left( \\sum_i^M b_iP_i(\\vec{x}) \\right) $$\n",
    "\n",
    "Finally, we can claim that we have _a priori_ knowledge of the parameters, whether it be through side-band analysis or external constraints, by including those constraints via some prior probability. Given no specific knowledge of the shape of that prior, we will consider the information we receive on the variables to be normally distributed and multiply the likelihood by those constraints\n",
    "\n",
    "$$\\mathcal{L}(\\vec{x}) = \\frac{\\text{e}^{-\\sum_i^Mb_i}}{N!} \\prod_j^N \\left( \\sum_i^M b_iP_i(\\vec{x}) \\right) \\frac{1}{\\sqrt{2\\pi \\sigma_j^2}}\\text{exp}\\left({\\frac{-(\\beta_i-b_i)^2}{2\\sigma_i^2}}\\right)$$\n",
    "\n",
    "A few definitions to simplify things:\n",
    "$$ \\lambda := \\sum_i^Mb_i $$\n",
    "\n",
    "Then then our objective function $\\mathcal{O} = -\\text{Ln}\\mathcal{L}$\n",
    "\n",
    "$$\\mathcal{O} = \\lambda + \\text{Ln}N! -\\sum_j^N\\text{Ln}\\left( \\sum_i^M b_iP_i(\\vec{x}) \\right) + \\sum_i^M \\left( \\frac{(\\beta_i-b_i)^2}{2\\sigma_i^2} + \\text{Ln}\\sqrt{2\\pi \\sigma_i} \\right)$$\n",
    "\n",
    "Finally, for a counting analysis we assume that an optimal set of cuts has been applied which optimizes the sensitivity to a particular parameter, which simplifies the likelihood such that\n",
    "$$ P_i(\\vec{x}) := 1 $$\n",
    "Also, because the shape of the likelihood space is independent of constant parameters, we can drop the $\\text{Ln}\\sqrt{2\\pi \\sigma_i}$ terms. We could also remove the $\\text{Ln}N!$ term as well, but for numerical stability we will keep it around, but use Sterling's approximation: $\\text{Ln}N! \\approx N\\text{Ln}N - N$. The remaining objective function we will thus use is:\n",
    "\n",
    "$$\\mathcal{O} = \\lambda - N\\text{Ln}\\lambda + N\\text{Ln}N - N + \\sum_i^M \\left( \\frac{(\\beta_i-b_i)^2}{2\\sigma_i^2} \\right)$$\n",
    "\n",
    "_Note: If the different values of $\\beta$ differ by orders of magnitude, it might be worth forming an affine invariant form of the likelihood, otherwise the $\\text{Ln}\\sqrt{2\\pi \\sigma_i}$ term should not matter_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"./watchfish.jl\")\n",
    "using .watchfish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 9.329882459084541e-13 at [10.000005115626163, 30.000002541040715, 10.000003649055872, 40.00002676166872, 20.00002282489499, 109.99993048690655, 9.999999424326171] after 2938 iterations\n",
      "results = Results(Opt(LN_SBPLX, 7), Model(Dict(\"Thing3\" => Component(\"Thing3\", 10.0, 20.0),\"Thing\" => Component(\"Thing\", 30.0, 10.0),\"Thing4\" => Component(\"Thing4\", 10.0, 10.0),\"Thing2\" => Component(\"Thing2\", 40.0, 30.0),\"Thing5\" => Component(\"Thing5\", 20.0, 30.0),\"Signal\" => Component(\"Signal\", 20.0, Inf),\"Thing6\" => Component(\"Thing6\", 10.0, 20.0)), 7), 9.329882459084541e-13, [10.000005115626163, 30.000002541040715, 10.000003649055872, 40.00002676166872, 20.00002282489499, 109.99993048690655, 9.999999424326171], 2938, [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [Inf, Inf, Inf, Inf, Inf, Inf, Inf])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Results(Opt(LN_SBPLX, 7), Model(Dict(\"Thing3\" => Component(\"Thing3\", 10.0, 20.0),\"Thing\" => Component(\"Thing\", 30.0, 10.0),\"Thing4\" => Component(\"Thing4\", 10.0, 10.0),\"Thing2\" => Component(\"Thing2\", 40.0, 30.0),\"Thing5\" => Component(\"Thing5\", 20.0, 30.0),\"Signal\" => Component(\"Signal\", 20.0, Inf),\"Thing6\" => Component(\"Thing6\", 10.0, 20.0)), 7), 9.329882459084541e-13, [10.000005115626163, 30.000002541040715, 10.000003649055872, 40.00002676166872, 20.00002282489499, 109.99993048690655, 9.999999424326171], 2938, [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [Inf, Inf, Inf, Inf, Inf, Inf, Inf])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = Model()\n",
    "add_component!(m, \"Signal\", 20.0; σ=Inf)\n",
    "add_component!(m, \"Thing\",  30.0; σ=10.0)\n",
    "add_component!(m, \"Thing2\", 40.0; σ=30.0)\n",
    "add_component!(m, \"Thing3\", 10.0; σ=20.0)\n",
    "add_component!(m, \"Thing4\",  10.0; σ=10.0)\n",
    "add_component!(m, \"Thing5\", 20.0; σ=30.0)\n",
    "add_component!(m, \"Thing6\", 10.0; σ=20.0)\n",
    "\n",
    "data = 230 #events\n",
    "results = run_fish(m, data)\n",
    "\n",
    "@show results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit converged after 2938 iterations\n",
      "Best fit at: -LnL = 9.329882459084541e-13\n",
      "Values of [10.000005115626163, 30.000002541040715, 10.000003649055872, 40.00002676166872, 20.00002282489499, 109.99993048690655, 9.999999424326171]\n",
      "results = \n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit converged after 2938 iterations\n",
      "Best fit at: -LnL = 9.329882459084541e-13\n",
      "Values of [10.000005115626163, 30.000002541040715, 10.000003649055872, 40.00002676166872, 20.00002282489499, 109.99993048690655, 9.999999424326171]\n"
     ]
    }
   ],
   "source": [
    "function Base.show(io::IO, m::Results)\n",
    "    compact = get(io, :compact, false)\n",
    "\n",
    "    if !compact\n",
    "        println(\"Fit converged after $(m.iterations) iterations\")\n",
    "        println(\"Best fit at: -LnL = $(m.min_objective)\")\n",
    "        println(\"Values of $(m.min_parameters)\")\n",
    "    end\n",
    "        #show(io, m.iterations)\n",
    "end\n",
    "\n",
    "@show results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup the tools\n",
    "using NLopt\n",
    "\n",
    "## Model variables\n",
    "M = 4\n",
    "β = (20.0, 30.0, 40.0, 10.0)\n",
    "σ = (Inf, 10.0, 30.0, 20.0) # Inf means unconstrained\n",
    "p0 = [10.0, 10.0, 10.0, 10.0]\n",
    "## Measurements\n",
    "N = 160\n",
    "\n",
    "function constraint(x, μ, σ)\n",
    "    if σ == Inf\n",
    "        return 0\n",
    "    end\n",
    "    if σ ≈ 0\n",
    "        return Inf\n",
    "    end\n",
    "    (x-μ)^2/2/σ^2\n",
    "end\n",
    "\n",
    "function objective(x::Vector, grad::Vector)\n",
    "    if length(grad)>0\n",
    "        grad = 2x\n",
    "    end\n",
    "    λ = sum(x)\n",
    "    ξ = sum( constraint.(x, β, σ) )\n",
    "    λ - N*log(λ) + N*log(N) - N + ξ\n",
    "end\n",
    "\n",
    "## Setup the optimizer, here we use nlopt with the sublex method\n",
    "## which works well for higher dimensionality on problems without\n",
    "## a known gradient.\n",
    "opt = Opt(:LN_SBPLX, M)\n",
    "opt.ftol_rel = 1e-4\n",
    "opt.min_objective = objective\n",
    "# Initial guess -- just need to be close-ish\n",
    "\n",
    "## Test constraint, move below\n",
    "#opt.lower_bounds = [1.0, -Inf, -Inf]\n",
    "#opt.upper_bounds = [1.0, Inf, Inf]\n",
    "## end test\n",
    "(minf, minx, ret) = optimize!(opt, p0)\n",
    "\n",
    "numevals = opt.numevals\n",
    "println(\"Got $minf at $minx after $numevals iterations (returned $ret)\")\n",
    "\n",
    "low  = copy(opt.lower_bounds)\n",
    "high = copy(opt.upper_bounds)\n",
    "best = copy(minx)\n",
    "minl = copy(minf)\n",
    "\n",
    "@show opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Profile likelihood of each parameter\n",
    "## WITHOUT a prior, we can take the normalized objective function\n",
    "using PyPlot\n",
    "using NLopt\n",
    "\n",
    "## Everything stored in results\n",
    "M = results.model.dims\n",
    "fig, ax = subplots(nrows=M, ncols=M)\n",
    "\n",
    "## We will be lazy for now and robust later. Lazy way, assume we know the range\n",
    "x = collect(0:1:100)\n",
    "y = collect(0:1:100)\n",
    "X = repeat( reshape(x, 1, :), length(y), 1)\n",
    "Y = repeat(y, 1, length(x))\n",
    "# What does a slice in space look like?\n",
    "# We can optimize again, with a constraint in place (bounds)\n",
    " \n",
    "for i in collect(1:M)\n",
    "    for j in collect(i:M)\n",
    "        # profile this parameter\n",
    "        nlow = copy(results.lower_bounds)\n",
    "        nhigh = copy(results.upper_bounds)\n",
    "        p0 = copy(results.min_parameters)\n",
    "        nll = []\n",
    "        if i != j\n",
    "            for a in x\n",
    "                for b in y\n",
    "                    nlow[i], nlow[j] = a, b\n",
    "                    nhigh[i], nhigh[j] = a, b\n",
    "                    results.opt.lower_bounds = nlow\n",
    "                    results.opt.upper_bounds = nhigh\n",
    "                    #p0 = copy(results.min_parameters)\n",
    "                    p0[i], p0[j] = a, b\n",
    "                    minf, minx, ret = optimize!(results.opt, p0)\n",
    "                    push!(nll, exp(-minf) )\n",
    "                end\n",
    "            end\n",
    "            Z = reshape(nll, length(x), length(y))\n",
    "            ax[j,i].contourf(x, y, Z)\n",
    "            continue\n",
    "        end\n",
    "        for a in x\n",
    "            nlow[i] = a\n",
    "            nhigh[i] = a\n",
    "            results.opt.lower_bounds = nlow\n",
    "            results.opt.upper_bounds = nhigh\n",
    "            p0[i] = a\n",
    "            #@show opt.lower_bounds[i]\n",
    "            minf, minx, ret = optimize!(results.opt, p0)\n",
    "            #@show ret minf p0\n",
    "            push!(nll, exp(-minf) )\n",
    "        end\n",
    "        ax[i,i].plot(x, nll)\n",
    "        ax[i,i].set_ylim(0, 1)\n",
    "    end\n",
    "end\n",
    "## Lets do a diagonal plot\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using NLopt\n",
    "using PyPlot\n",
    "function profileSingle(idx, results; stop=6, step=1.0)\n",
    "    # Go left\n",
    "    x_left = []\n",
    "    nll_left = []\n",
    "    nlow = copy(results.lower_bounds)\n",
    "    nhigh = copy(results.upper_bounds)\n",
    "    p0 = copy(results.min_parameters)\n",
    "    val = 0\n",
    "    xeval = p0[idx]\n",
    "    while (val-results.min_objective) < stop\n",
    "        nlow[idx] = xeval\n",
    "        nhigh[idx] = xeval\n",
    "        results.opt.lower_bounds = nlow\n",
    "        results.opt.upper_bounds = nhigh\n",
    "        p0[idx] = xeval\n",
    "        val, minx, ret = optimize!(results.opt, p0)\n",
    "        push!(nll_left, exp(-(val-results.min_objective)) )\n",
    "        push!(x_left, xeval)\n",
    "        xeval -= step\n",
    "    end\n",
    "    # Go right\n",
    "    x_right = []\n",
    "    nll_right = []\n",
    "    nlow = copy(results.lower_bounds)\n",
    "    nhigh = copy(results.upper_bounds)\n",
    "    p0 = copy(results.min_parameters)\n",
    "    val = 0\n",
    "    xeval = p0[idx]\n",
    "    while (val-results.min_objective) < stop\n",
    "        nlow[idx] = xeval\n",
    "        nhigh[idx] = xeval\n",
    "        results.opt.lower_bounds = nlow\n",
    "        results.opt.upper_bounds = nhigh\n",
    "        p0[idx] = xeval\n",
    "        val, minx, ret = optimize!(results.opt, p0)\n",
    "        push!(nll_right, exp(-(val-results.min_objective)) )\n",
    "        push!(x_right, xeval)\n",
    "        xeval += step\n",
    "    end\n",
    "    x = append!(reverse(x_left), x_right)\n",
    "    nll = append!(reverse(nll_left), nll_right)\n",
    "    return x, nll\n",
    "end\n",
    "\n",
    "function uncertainty(likelihood_x, likelihood_y, α ; mode=\"FC\")\n",
    "    # Mode can be FC (Feldman-Cousins), Mode-centered, mean-centered, left, right, etc.\n",
    "    # Return cumulative, x_low, x_high\n",
    "    likelihood_y = likelihood_y / sum(likelihood_y)\n",
    "    cumulative = cumsum(likelihood_y)\n",
    "    β = 1 - α\n",
    "    @show β\n",
    "    if mode == \"FC\"\n",
    "        cy = sortperm(likelihood_y; rev=true)\n",
    "        cum_y = cumsum(likelihood_y[cy])\n",
    "        cum_x = likelihood_x[cy]\n",
    "        region = cum_x[ cum_y .<= α ]\n",
    "        left = minimum(region)\n",
    "        right = maximum(region)\n",
    "    end\n",
    "    left, right\n",
    "end\n",
    "\n",
    "x, y = profileSingle(4, results; stop=6, step=1)\n",
    "l, r = uncertainty(x, y, 0.90)\n",
    "\n",
    "@show l r\n",
    "plot(x, y)\n",
    "axvline(l, color=\"red\")\n",
    "axvline(r, color=\"red\")\n",
    "ylim(bottom=0)\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function profileSingle(idx, opt, bestfit, minfit; stop=6, step=1.0)\n",
    "    # Go left\n",
    "    x_left = []\n",
    "    nll_left = []\n",
    "    nlow = copy(opt.lower_bounds)\n",
    "    nhigh = copy(opt.upper_bounds)\n",
    "    p0 = copy(bestfit)\n",
    "    val = 0\n",
    "    xeval = p0[idx]\n",
    "    while (val-minfit) < stop\n",
    "        nlow[idx] = xeval\n",
    "        nhigh[idx] = xeval\n",
    "        opt.lower_bounds = nlow\n",
    "        opt.upper_bounds = nhigh\n",
    "        p0[idx] = xeval\n",
    "        val, minx, ret = optimize!(opt, p0)\n",
    "        push!(nll_left, exp(-(val-minfit)) )\n",
    "        push!(x_left, xeval)\n",
    "        xeval -= step\n",
    "    end\n",
    "    # Go right\n",
    "    x_right = []\n",
    "    nll_right = []\n",
    "    nlow = copy(opt.lower_bounds)\n",
    "    nhigh = copy(opt.upper_bounds)\n",
    "    p0 = copy(bestfit)\n",
    "    val = 0\n",
    "    xeval = p0[idx]\n",
    "    while (val-minfit) < stop\n",
    "        nlow[idx] = xeval\n",
    "        nhigh[idx] = xeval\n",
    "        opt.lower_bounds = nlow\n",
    "        opt.upper_bounds = nhigh\n",
    "        p0[idx] = xeval\n",
    "        val, minx, ret = optimize!(opt, p0)\n",
    "        push!(nll_right, exp(-(val-minfit)) )\n",
    "        push!(x_right, xeval)\n",
    "        xeval += step\n",
    "    end\n",
    "    x = append!(reverse(x_left), x_right)\n",
    "    nll = append!(reverse(nll_left), nll_right)\n",
    "    return x, nll\n",
    "end\n",
    "\n",
    "function uncertainty(likelihood_x, likelihood_y, α ; mode=\"FC\")\n",
    "    # Mode can be FC (Feldman-Cousins), Mode-centered, mean-centered, left, right, etc.\n",
    "    # Return cumulative, x_low, x_high\n",
    "    likelihood_y = likelihood_y / sum(likelihood_y)\n",
    "    cumulative = cumsum(likelihood_y)\n",
    "    β = 1 - α\n",
    "    @show β\n",
    "    if mode == \"FC\"\n",
    "        cy = sortperm(likelihood_y; rev=true)\n",
    "        cum_y = cumsum(likelihood_y[cy])\n",
    "        cum_x = likelihood_x[cy]\n",
    "        region = cum_x[ cum_y .<= α ]\n",
    "        left = minimum(region)\n",
    "        right = maximum(region)\n",
    "    end\n",
    "    left, right\n",
    "end\n",
    "\n",
    "x, y = profileSingle(1, opt, best, minl; stop=6, step=1)\n",
    "l, r = uncertainty(x, y, 0.90)\n",
    "\n",
    "@show l r\n",
    "plot(x, y)\n",
    "axvline(l, color=\"red\")\n",
    "axvline(r, color=\"red\")\n",
    "ylim(bottom=0)\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing bits, write and move\n",
    "mutable struct Component\n",
    "    name::String\n",
    "    μ::Float64\n",
    "    σ::Float64\n",
    "end\n",
    "\n",
    "mutable struct Model\n",
    "    component_dict::Dict{String, Component}\n",
    "    ndims::Int64\n",
    "    function Model()\n",
    "        new(Dict{String, Component}(), 0)\n",
    "    end\n",
    "end\n",
    "\n",
    "function show(m::Model)\n",
    "    m.component_dict\n",
    "end\n",
    "\n",
    "function add_component!(m::Model, name::String, μ; σ=Inf)\n",
    "    get!(m.component_dict, name, Component(name, μ, σ))\n",
    "    m.ndims = length(m.component_dict)\n",
    "end\n",
    "    \n",
    "m = Model()\n",
    "add_component!(m, \"Signal\", 1.7; σ=Inf)\n",
    "\n",
    "@show m"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.2.0",
   "language": "julia",
   "name": "julia-1.2"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
