{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Poisson likelihood calculator\n",
    "\n",
    "Given $M$ components, each with an estimated rate $\\vec{\\beta}$ determined by a normal distribution with uncertainty $\\vec{\\sigma}$, calculate the confidence itervals and perform a hypothesis tests for each parameter $b$.\n",
    "\n",
    "Nominally each event corresponds to a set of observables $\\vec{x}$ of $N$ measurements, for any given measurement, the probability for that particular measurement to come from a particular components is given by\n",
    "\n",
    "$$ P_i(\\vec{x}) \\tag{1}$$\n",
    "\n",
    "The prior probability is then formed through a combination of these components such that the total probability is \n",
    "\n",
    "$$ \\mathbf{P} = \\sum_i^M P_i(\\vec{x}) \\tag{2}$$\n",
    "\n",
    "The likelihood for a full data set of $N$ measurements is the product of each event total probability\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\vec{x}) = \n",
    "\\prod_j^N \\left( \\sum_i^M b_iP_i(\\vec{x}) \\right) / \\sum_i^Mb_i \n",
    "\\tag{3}\n",
    "$$\n",
    "\n",
    "We can extend the likelihood by proclaiming that each components as well as the sum of components are simply a stochastic process, produces the extended likelihood:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\vec{x}) = \n",
    "\\frac{\\text{e}^{-\\sum_i^Mb_i}}{N!} \\prod_j^N \\left( \\sum_i^M b_iP_i(\\vec{x}) \\right) \n",
    "\\tag{4}\n",
    "$$\n",
    "\n",
    "Finally, we can claim that we have _a priori_ knowledge of the parameters, whether it be through side-band analysis or external constraints, by including those constraints via some prior probability. Given no specific knowledge of the shape of that prior, we will consider the information we receive on the variables to be normally distributed and multiply the likelihood by those constraints\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\vec{x}) = \n",
    "\\frac{\\text{e}^{-\\sum_i^Mb_i}}{N!} \n",
    "\\prod_j^N \\left( \\sum_i^M b_iP_i(\\vec{x}) \\right) \n",
    "\\frac{1}{\\sqrt{2\\pi \\sigma_j^2}}\n",
    "\\text{exp}\\left({\\frac{-(\\beta_i-b_i)^2}{2\\sigma_i^2}}\\right)\n",
    "\\tag{5}\n",
    "$$\n",
    "\n",
    "A few definitions to simplify things:\n",
    "$$ \\lambda := \\sum_i^Mb_i \\tag{6}$$\n",
    "\n",
    "Then then our objective function $\\mathcal{O} = -\\text{Ln}\\mathcal{L}$\n",
    "\n",
    "$$\n",
    "\\mathcal{O} = \\lambda + \\text{Ln}N! \n",
    "-\\sum_j^N\\text{Ln}\\left( \\sum_i^M b_iP_i(\\vec{x}) \\right) \n",
    "+ \\sum_i^M \\left( \\frac{(\\beta_i-b_i)^2}{2\\sigma_i^2} \n",
    "    + \\text{Ln}\\sqrt{2\\pi \\sigma_i} \\right)\n",
    "\\tag{7}\n",
    "$$\n",
    "\n",
    "Finally, for a counting analysis we assume that an optimal set of cuts has been applied which optimizes the sensitivity to a particular parameter, which simplifies the likelihood such that\n",
    "\n",
    "$$ P_i(\\vec{x}) := 1 \\tag{8}$$\n",
    "\n",
    "Also, because the shape of the likelihood space is independent of constant parameters, we can drop the $\\text{Ln}\\sqrt{2\\pi \\sigma_i}$ terms. We could also remove the $\\text{Ln}N!$ term as well, but for numerical stability we will keep it around, but use Sterling's approximation: $\\text{Ln}N! \\approx N\\text{Ln}N - N$. The remaining objective function we will thus use is:\n",
    "\n",
    "$$\n",
    "\\mathcal{O} = \\lambda - N\\text{Ln}\\lambda + N\\text{Ln}N - N \n",
    "    + \\sum_i^M \\left( \\frac{(\\beta_i-b_i)^2}{2\\sigma_i^2} \\right)\n",
    "\\tag{9}\n",
    "$$\n",
    "\n",
    "_Note: If the different values of $\\beta$ differ by orders of magnitude, it might be worth forming an affine invariant form of the likelihood, otherwise the $\\text{Ln}\\sqrt{2\\pi \\sigma_i}$ term should not matter_\n",
    "\n",
    "[Profile Likelihood](https://www.stat.tamu.edu/~suhasini/teaching613/chapter3.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"./src/WatchFish.jl\")\n",
    "using .WatchFish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building\n",
    "Three structures are used to build, test, and fit models. These are the `Model`, `Component`, and `Result`--all of which are mutable.\n",
    "\n",
    "We begin by initializing a default `Model` and one-by-one use `add_component!` to modify the `Model`.\n",
    "\n",
    "The intention is to provide multiple ways to evaluate the uncertainties, the default here is to compute the profile likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the fish model we take the generic likelihood and enforce poisson statistics, thereby\n",
    "# eliminating correlations between parameters.\n",
    "\n",
    "m = Model()\n",
    "\n",
    "add_component!(m, \"Signal\", 0.0; σ=Inf)\n",
    "add_component!(m, \"Bkg 1\",  30.0; σ=10.0)\n",
    "#add_component!(m, \"Bkg 2\", 40.0; σ=45.0)\n",
    "#add_component!(m, \"Bkg 3\", 12.0; σ=12.0)\n",
    "\n",
    "data = 25 #events\n",
    "results = run_fish!(m, data)\n",
    "\n",
    "println(\"Found min, profiling now\")\n",
    "\n",
    "# Results stored in a DataFrame.jl\n",
    "compute_profiled_uncertainties!(results, 0.68)\n",
    "pretty_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using PyPlot\n",
    "plt.style.use(\"watchman\")\n",
    "\n",
    "interval_plot(results, \"Signal\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Profile likelihood of each parameter\n",
    "## WITHOUT a prior, we can take the normalized objective function\n",
    "using PyPlot\n",
    "using NLopt\n",
    "\n",
    "function correlation_plots(results; steps=100)\n",
    "    ## Everything stored in results\n",
    "    M = results.model.dims\n",
    "    fig, ax = subplots(nrows=M, ncols=M)\n",
    "\n",
    "    ## We will be lazy for now and robust later. Lazy way, assume we know the range\n",
    "    # What does a slice in space look like?\n",
    "    # We can optimize again, with a constraint in place (bounds)\n",
    "    levels = exp.(-[Inf,11.83,6.18,2.3,0].^0.5./2)\n",
    "    \n",
    "    for (i, (k,v)) in enumerate(results.model.component_dict)\n",
    "        for (j, (l,w)) in enumerate(results.model.component_dict)\n",
    "            if j < M\n",
    "                ax[j, i].set_xticklabels([])\n",
    "            end\n",
    "            if i > 1\n",
    "                ax[j, i].set_yticklabels([])\n",
    "            end\n",
    "            if i > j\n",
    "                ax[j, i].axis(\"off\")\n",
    "                ax[j, i].text(0,0.5,\"hi\")\n",
    "                continue\n",
    "            end\n",
    "            # profile this parameter\n",
    "            nlow = copy(results.lower_bounds)\n",
    "            nhigh = copy(results.upper_bounds)\n",
    "            p0 = copy(results.min_parameters)\n",
    "            nll = []\n",
    "            if i != j\n",
    "                A = range(minimum(v.likelihood_x), length=steps, stop=maximum(v.likelihood_x)) |> collect\n",
    "                B = range(minimum(w.likelihood_x), length=steps, stop=maximum(w.likelihood_x)) |> collect\n",
    "                for a in A\n",
    "                    for b in B\n",
    "                        nlow[i], nlow[j] = a, b\n",
    "                        nhigh[i], nhigh[j] = a, b\n",
    "                        results.opt.lower_bounds = nlow\n",
    "                        results.opt.upper_bounds = nhigh\n",
    "                        #p0 = copy(results.min_parameters)\n",
    "                        p0[i], p0[j] = a, b\n",
    "                        minf, minx, ret = optimize!(results.opt, p0)\n",
    "                        if minf == 0\n",
    "                            minf = 100\n",
    "                        end\n",
    "                        push!(nll, exp(-minf) )\n",
    "                    end\n",
    "                end\n",
    "                nll = nll / maximum(nll)\n",
    "                Z = reshape(nll, length(A), length(B))\n",
    "                \n",
    "                ## Given Z\n",
    "                \n",
    "                ax[j,i].contourf(A, B, Z, levels=levels)\n",
    "                continue\n",
    "            end\n",
    "            ax[i, i].plot( v.likelihood_x, v.likelihood_y )\n",
    "            ax[i, i].set_title(k)\n",
    "            ax[i,i].set_ylim(0, 1.2)\n",
    "        end\n",
    "    end\n",
    "end\n",
    "## Lets do a diagonal plot\n",
    "\n",
    "correlation_plots(results)\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate fake experiments; two set => Positive, Negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cowen Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refactor code into sub-files, project.toml, mybinder, etc ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time dependent systematic (maybe)"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.2.0",
   "language": "julia",
   "name": "julia-1.2"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
